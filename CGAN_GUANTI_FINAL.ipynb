{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo ora il Dataset di Training di Accessories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ad6e76a886522b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dato la grande mole di immagini e categorie differenti per quanto riguarda gli accessories scegliamo semplicemente una delle tipologie di accessories, per esempio i guanti (gloves) e creiamo un dataset solo di gloves"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "889e2d2d91488e43"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/marce/Desktop/polyvore_outfits/images/101212547.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/101628351.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/102591988.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/104037796.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/104669815.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/10776218.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/10802509.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/112378503.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/112607812.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/112776517.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/113099043.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/114319770.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/114762168.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/115642170.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/116391560.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/116992461.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/117405531.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/117474713.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118169985.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118339602.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118427814.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118431240.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118496155.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118584879.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118873856.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118873965.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119042253.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119087811.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/1191384.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119146029.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119148196.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119498179.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119592293.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119691662.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119863260.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120019743.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120258048.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120492535.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120503975.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120542887.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120593477.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/120609695.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121047451.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121138423.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121184321.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121263838.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121388508.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121590764.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121648578.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122054152.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122097029.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122164024.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122208702.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122326880.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122352060.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/122391717.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/123161171.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/123403442.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/123738248.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/123784574.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/124557434.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/124758534.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/125471692.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/125741097.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/125987838.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/126041762.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/126627961.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/126639584.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/127165662.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/127186090.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/127249957.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/129505873.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/129964383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/134417632.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/136145199.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/136471910.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/137800820.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/138019520.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/138335334.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/13868276.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/140611822.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/141328864.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/143790619.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/143929767.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/144019665.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/144213191.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/144582606.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/144721036.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/145290383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146236545.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146272922.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146456230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146522372.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146701120.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146705254.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146707246.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146737232.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147257105.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147323571.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147501704.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147518024.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147734194.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/147988308.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148125350.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148193411.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148293424.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148410673.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148482437.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148485637.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148681170.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148690019.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148691574.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148693296.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148807551.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149055603.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149091265.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149172639.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149175149.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149613086.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149721366.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149804304.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149847818.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150135730.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150291857.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150391529.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150392398.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150423173.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150473602.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150479880.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150536187.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150551691.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150715777.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150860794.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150953512.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151096673.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151098739.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151214058.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151221354.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151323253.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151326787.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151366142.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151368183.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151368457.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151688918.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151770728.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151772829.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151971584.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151988261.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152013812.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152085573.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152171551.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152250152.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152275082.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152290233.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152354599.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152516464.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152729065.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152899979.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152940896.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152998750.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153029890.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153090500.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153150192.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153155641.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153327429.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153328526.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153328863.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153330632.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153330986.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153351227.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153656844.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153688590.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153814537.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153975917.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154184741.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154201319.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154398231.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154410710.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154438836.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154472821.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154473676.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154594733.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154628464.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154696784.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154761898.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154771047.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154924690.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155012082.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155035998.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155036720.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155036869.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155062677.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155594596.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155600785.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155695998.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155728727.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155740669.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155952052.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156022413.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156096369.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156101760.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156322564.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156384405.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156396784.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156912836.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157005187.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157095311.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157173938.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157339323.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157461068.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157468165.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157519825.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157574055.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157788089.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157939484.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157975176.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158014483.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158108712.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158424673.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158432286.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158796628.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158878614.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/159018898.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/159645191.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/159793868.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/160029789.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/160490668.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/162562609.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/162736154.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/16574276.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/165765175.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/166639249.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/168878347.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/169020555.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/169020666.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/169199036.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/170355567.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/172123383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/172158987.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/172913288.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/174683194.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/174853589.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/174870639.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/176462915.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/176904859.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/177263574.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/177675185.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/177871030.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/177988948.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/179053235.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/179530620.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/179530648.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180180049.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180256666.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180259044.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180277873.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180746608.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180754515.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181089265.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181140878.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181140893.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181344768.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181365230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181751347.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181774767.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182095522.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182134879.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182252303.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182336010.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182343339.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182387211.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182449661.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182449669.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182450185.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182551457.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182664376.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182664387.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182691359.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182701916.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182702378.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183008167.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183247109.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183247631.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183262230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183262294.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183280024.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183285271.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183391765.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183416768.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183499111.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183574725.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/18390006.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183985126.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184017658.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184022034.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184022039.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184082224.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184083219.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184083309.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184116387.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184383814.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184494490.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184538311.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184582873.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184607807.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184607875.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184613474.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184681012.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184690719.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184695024.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184722990.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184800743.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/18491674.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185047079.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185293541.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185499150.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185527580.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185529353.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185534410.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185744772.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185822785.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185922531.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185926365.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185926376.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185936806.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185936822.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185937639.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185943155.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186039217.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186068069.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186068834.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186144304.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186144540.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186145122.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186148050.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186164225.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186164228.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186175974.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186371402.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186416630.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186517292.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186583226.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186587413.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186632791.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186678221.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186686835.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186700805.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186766626.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187130796.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187271869.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187283852.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187291632.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187312770.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187526336.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187614877.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187976230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188103568.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188111566.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188111678.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188122255.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188233429.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188382411.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188438892.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188450367.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188453540.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188453627.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188453750.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188458890.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188638650.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188640797.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188643250.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188816672.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188855140.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189077450.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189264772.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189443787.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189543948.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189559906.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189699377.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189699391.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189700722.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189700742.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189702664.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189770819.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190004730.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190146469.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190250639.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190287018.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190291277.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190300934.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190332823.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190343835.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190489381.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190823438.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190882431.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190910073.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191069401.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191647533.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191648627.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191802539.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191842107.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191907458.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191907542.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191907649.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192031844.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192175427.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192367099.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192534403.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192534418.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192544222.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192571063.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192577601.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192632571.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192728783.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192771432.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192882961.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192935923.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192945227.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193018061.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193091139.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193239462.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193270274.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193299387.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193342039.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193372946.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193389383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193406219.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193436831.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193620268.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193695509.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193699465.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193755704.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193853190.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194036200.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194155106.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194304301.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194361935.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194442422.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194546611.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195102261.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195214308.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195371685.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195530664.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195629074.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195802970.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195866847.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/195871723.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196292253.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196311098.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196340850.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196443287.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/199166289.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/199700363.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/199703442.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/199963191.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/200633032.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/200904504.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/201669221.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/201682660.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/201905047.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/202093059.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/202225287.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/204645882.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/205889417.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/206574100.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207349095.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207360662.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207395288.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207815629.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207853017.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/208176854.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/208194173.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/209309557.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/211078764.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/211572808.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/211729237.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/212067777.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/212612807.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/212718405.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/213568439.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/213802301.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/213933163.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/214335205.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/215102961.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/24089591.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/24626484.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/26252813.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/3732051.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/37969152.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/4397964.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/44114982.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/44854971.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/45234312.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/45702346.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/47689030.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/49918533.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/50511962.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/5078143.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/54568547.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/59459033.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/60307149.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/61343964.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/64356040.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/65048299.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/65458798.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/65519035.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/66010705.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/66116759.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/66326060.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/67942926.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/68115224.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/68757308.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/69658769.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/70326811.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/70327076.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/71133230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/71325042.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/73767438.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/7405874.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/75087504.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/76956435.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/7697377.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/82630743.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/83170734.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/85211060.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/8550510.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/88524149.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/89194323.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/89286195.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/89292716.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/89432048.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/90311587.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/90586498.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/90913971.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/91385240.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/92453229.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/92534321.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/92672726.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/92708998.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/92860597.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/93084662.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/93515907.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/94006674.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/94636651.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/94663777.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/95464555.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/95726258.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/95792940.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/96493509.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98078746.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98373434.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98585254.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98710795.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/99313918.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/99828386.jpg']\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 300, 300, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definisci il percorso della cartella contenente le immagini\n",
    "image_directory = 'C:/Users/marce/Desktop/polyvore_outfits/images/'\n",
    "\n",
    "# Carica il file JSON delle etichette\n",
    "json_file_path = 'Dataset/train_set_acc.json'\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    label_data = json.load(json_file)\n",
    "\n",
    "# Crea un dizionario per mappare item_id alle etichette semantiche\n",
    "item_id_to_label = {}\n",
    "for entry in label_data:\n",
    "    for item in entry['items']:\n",
    "        item_id = item['item_id']\n",
    "        category_id = item['category_id']\n",
    "        item_id_to_label[item_id] = category_id\n",
    "\n",
    "# Creare una lista di tuple (nome dell'immagine, etichetta semantica)\n",
    "image_label_pairs = []\n",
    "for root, _, files in os.walk(image_directory):\n",
    "    for file in files:\n",
    "        # Estrai l'item_id dal nome dell'immagine\n",
    "        item_id = os.path.splitext(file)[0]\n",
    "        category_id = item_id_to_label.get(item_id, None)\n",
    "        if category_id == \"gloves\":\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_label_pairs.append((image_path, category_id))\n",
    "\n",
    "# Creare un dataset TensorFlow dalla lista di coppie immagine-etichetta\n",
    "image_paths, labels = zip(*image_label_pairs)\n",
    "image_paths = list(image_paths)\n",
    "print(image_paths)\n",
    "labels = list(labels)\n",
    "\n",
    "# Converte le etichette semantiche in interi (necessario per poi poter effettuare i merge all'interno del generatore e del discriminatore)\n",
    "label_mapping = {\"gloves\": 0}\n",
    "def label_to_integer(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "labels = [label_to_integer(label) for label in labels]\n",
    "\n",
    "# Creare un dataset TensorFlow\n",
    "batch_size = 32\n",
    "image_size = (300, 300)\n",
    "\n",
    "def load_and_preprocess_image(image_path, label):\n",
    "    # Carica e preelabora l'immagine\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    return image, label\n",
    "\n",
    "# Creare un dataset da coppie immagine-etichetta\n",
    "dataset_gloves = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_gloves = dataset_gloves.map(load_and_preprocess_image)\n",
    "\n",
    "# Per aggiungere il batch size alla dimensione del tensore per il dataset\n",
    "dataset_gloves = dataset_gloves.batch(batch_size)\n",
    "\n",
    "# Stampa il dataset\n",
    "print(dataset_gloves)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:05:06.948292Z",
     "start_time": "2023-09-30T08:05:04.213750800Z"
    }
   },
   "id": "9de8f9918c515eec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lavoriamo sul dataset di validation come abbiamo lavorato sul dataset di training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf1c31cb7bf852a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/marce/Desktop/polyvore_outfits/images/114072741.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/115331633.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118591640.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119827847.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/121099166.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/125696781.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/129972226.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/130403543.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/136382766.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/140815738.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/14250562.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/143790619.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146456230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146600417.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148073915.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148681164.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148855708.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150054919.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152354599.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154924690.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155740669.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157095311.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157853559.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/164301270.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/169020665.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/169020666.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/174385721.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/179390073.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182456933.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182763751.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183247416.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183247600.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183304123.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184022401.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185181837.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185811165.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185922509.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186183457.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188453657.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188765843.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189559906.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189700722.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190823438.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191644560.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192031844.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193389383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193806282.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/200025603.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207349095.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/20892110.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/213568439.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/50650664.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/69965928.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/70572698.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/82630743.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98710795.jpg']\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 300, 300, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definisci il percorso della cartella contenente le immagini\n",
    "image_directory = 'C:/Users/marce/Desktop/polyvore_outfits/images/'\n",
    "\n",
    "# Carica il file JSON delle etichette\n",
    "json_file_path = 'Dataset/valid_set_acc.json'\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    label_data = json.load(json_file)\n",
    "\n",
    "# Crea un dizionario per mappare item_id alle etichette semantiche\n",
    "item_id_to_label = {}\n",
    "for entry in label_data:\n",
    "    for item in entry['items']:\n",
    "        item_id = item['item_id']\n",
    "        category_id = item['category_id']\n",
    "        item_id_to_label[item_id] = category_id\n",
    "\n",
    "# Creare una lista di tuple (nome dell'immagine, etichetta semantica)\n",
    "image_label_pairs = []\n",
    "for root, _, files in os.walk(image_directory):\n",
    "    for file in files:\n",
    "        # Estrai l'item_id dal nome dell'immagine\n",
    "        item_id = os.path.splitext(file)[0]\n",
    "        category_id = item_id_to_label.get(item_id, None)\n",
    "        if category_id == \"gloves\":\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_label_pairs.append((image_path, category_id))\n",
    "\n",
    "# Creare un dataset TensorFlow dalla lista di coppie immagine-etichetta\n",
    "image_paths, labels = zip(*image_label_pairs)\n",
    "image_paths = list(image_paths)\n",
    "print(image_paths)\n",
    "labels = list(labels)\n",
    "\n",
    "# Converte le etichette semantiche in interi (necessario per poi poter effettuare i merge all'interno del generatore e del discriminatore)\n",
    "label_mapping = {\"gloves\": 0}\n",
    "def label_to_integer(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "labels = [label_to_integer(label) for label in labels]\n",
    "\n",
    "# Creare un dataset TensorFlow\n",
    "batch_size = 32\n",
    "image_size = (300, 300)\n",
    "\n",
    "def load_and_preprocess_image(image_path, label):\n",
    "    # Carica e preelabora l'immagine\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    return image, label\n",
    "\n",
    "# Creare un dataset da coppie immagine-etichetta\n",
    "dataset_gloves_val = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_gloves_val = dataset_gloves_val.map(load_and_preprocess_image)\n",
    "\n",
    "# Per aggiungere il batch size alla dimensione del tensore per il dataset\n",
    "dataset_gloves_val = dataset_gloves_val.batch(batch_size)\n",
    "\n",
    "# Stampa il dataset\n",
    "print(dataset_gloves_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:05:39.172464Z",
     "start_time": "2023-09-30T08:05:36.936128Z"
    }
   },
   "id": "5b04c8b08b429662"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creiamo il Dataset del test set che riguarda la category_id gloves "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2e37ea9dfaf43ef"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/marce/Desktop/polyvore_outfits/images/100005237.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/113583626.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/114319770.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/115936986.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/11652684.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118431477.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118496150.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118496155.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/118698657.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119592293.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119863260.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/119932215.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/124285281.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/136145199.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146236545.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/146456230.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/148139869.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/149055603.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150250360.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150383540.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150384187.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150479880.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/150706556.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151096673.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/151366786.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152085573.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/152729065.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153054589.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153330986.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153658060.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/153975953.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154409890.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154696784.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154771047.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/154924690.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155312114.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/155662871.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/156002800.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/157939550.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158014483.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/158409085.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/16684891.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/174853874.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/180256666.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181107801.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181109657.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181128525.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181251476.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/181467742.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182664387.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182702378.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/182767585.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183261891.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183261951.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183369146.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/183499111.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184028873.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/184083309.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185640530.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185811165.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185822785.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185855160.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/185914715.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186144540.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186155560.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/186766626.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/187918833.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188215923.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188233429.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188638670.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188643209.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/188940302.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189069107.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189077450.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189699425.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/189700819.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190291277.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190298728.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/190888106.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191644560.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/191948292.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192031844.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192449294.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192534370.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192534403.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192534418.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192771432.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/192935923.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193088760.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193278047.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193389383.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193622917.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/193695509.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/194361935.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196232729.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/196775166.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/200647910.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/201905047.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/202093059.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/205525307.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/207349095.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/213771744.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/2335179.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/24524957.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/40706967.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/43665671.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/46848331.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/47260332.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/61845065.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/64953491.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/70938114.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/86545644.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/90129094.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/95344392.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/96014574.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/9644729.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/97288838.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/97907892.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98210952.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/98710795.jpg', 'C:/Users/marce/Desktop/polyvore_outfits/images/99538239.jpg']\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 300, 300, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Definisci il percorso della cartella contenente le immagini\n",
    "image_directory = 'C:/Users/marce/Desktop/polyvore_outfits/images/'\n",
    "\n",
    "# Carica il file JSON delle etichette\n",
    "json_file_path = 'Dataset/test_set_acc.json'\n",
    "\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    label_data = json.load(json_file)\n",
    "\n",
    "# Crea un dizionario per mappare item_id alle etichette semantiche\n",
    "item_id_to_label = {}\n",
    "for entry in label_data:\n",
    "    for item in entry['items']:\n",
    "        item_id = item['item_id']\n",
    "        category_id = item['category_id']\n",
    "        item_id_to_label[item_id] = category_id\n",
    "\n",
    "# Creare una lista di tuple (nome dell'immagine, etichetta semantica)\n",
    "image_label_pairs = []\n",
    "for root, _, files in os.walk(image_directory):\n",
    "    for file in files:\n",
    "        # Estrai l'item_id dal nome dell'immagine\n",
    "        item_id = os.path.splitext(file)[0]\n",
    "        category_id = item_id_to_label.get(item_id, None)\n",
    "        if category_id == \"gloves\":\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_label_pairs.append((image_path, category_id))\n",
    "\n",
    "# Creare un dataset TensorFlow dalla lista di coppie immagine-etichetta\n",
    "image_paths, labels = zip(*image_label_pairs)\n",
    "image_paths = list(image_paths)\n",
    "print(image_paths)\n",
    "labels = list(labels)\n",
    "\n",
    "# Converte le etichette semantiche in interi \n",
    "label_mapping = {\"gloves\": 0}\n",
    "def label_to_integer(label):\n",
    "    return label_mapping[label]\n",
    "\n",
    "labels = [label_to_integer(label) for label in labels]\n",
    "\n",
    "# Creare un dataset TensorFlow\n",
    "batch_size = 32\n",
    "image_size = (300, 300)\n",
    "\n",
    "def load_and_preprocess_image(image_path, label):\n",
    "    # Carica e preelabora l'immagine\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    return image, label\n",
    "\n",
    "# Creare un dataset da coppie immagine-etichetta\n",
    "dataset_gloves_test = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_gloves_test = dataset_gloves_test.map(load_and_preprocess_image)\n",
    "\n",
    "# Per aggiungere il batch size alla dimensione del tensore per il dataset\n",
    "dataset_gloves_test = dataset_gloves_test.batch(batch_size)\n",
    "\n",
    "# Stampa il dataset\n",
    "print(dataset_gloves_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:05:48.961848600Z",
     "start_time": "2023-09-30T08:05:46.995572400Z"
    }
   },
   "id": "f24a7babef2cf5c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carichiamo l'embedding qualora fosse già stato caricato"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67bc15d901f1690"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6698174   0.36767587  2.0241685   0.59659916  0.6344788   0.02297916\n",
      "   0.6140562   0.32278645  0.2504632  -0.6127753  -0.4668099   0.05412364\n",
      "  -1.2268093  -0.23452778  0.31057438 -0.07198033  0.21593237 -0.55607826\n",
      "   0.10858711  1.7606628  -0.7215787  -0.07988317 -1.3390868  -0.33724254\n",
      "  -0.10062222  0.5519891   2.221586   -0.11137453  0.62013286 -0.7614147\n",
      "   0.44494778 -0.07807393 -1.0250447   0.5582578  -0.80462873  0.57461697\n",
      "   0.08267806 -1.3114728  -0.7518677   0.22460711  1.8143798   0.02875041\n",
      "  -0.16374594  0.9635703  -0.9323846   0.63075733 -0.4927859   2.5482798\n",
      "   0.25572893 -1.1241668  -0.27286592  0.8137388  -1.4529666   0.12695476\n",
      "  -1.9686215   0.15296736 -1.482131    0.07811125  1.1749024  -0.54084545\n",
      "  -0.58014643 -0.4272632  -0.85797626  0.11588039 -0.40775523  1.0385048\n",
      "  -0.08945616  0.14564554  0.07501117 -0.48413223 -0.30717677 -0.19494504\n",
      "   0.30911177 -0.57293016 -0.6120647   0.43311933 -0.5307508  -0.734938\n",
      "  -0.8050627  -1.5378573  -0.47905245 -0.9906199  -0.05237616 -0.7126654\n",
      "   0.6725428   0.9802146  -1.2689995  -0.5596127   0.24834897 -0.7211162\n",
      "  -0.80236095 -0.28756604  1.2398614   0.39326572 -1.06237     1.2258954\n",
      "   0.5506661   0.191221   -2.6231465   0.42328066  0.58008176 -0.96136343\n",
      "   0.50810754 -1.2518668   1.0318357  -1.1004219   0.1412485  -0.79358864\n",
      "   0.85427994  1.317386    1.1610643   0.5954114   2.293097   -0.5467718\n",
      "  -1.1352992   0.38247126 -1.1312469  -1.2681321   1.6919221   0.5658218\n",
      "   0.36378175 -0.47135553 -0.43389356  0.5758003   0.5486006   0.7501549\n",
      "   0.5205463  -0.89677703]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Carica l'embedding dal file\n",
    "embedding = np.load('embedding.npy')\n",
    "\n",
    "# Ora puoi utilizzare l'embedding come desideri nel tuo notebook\n",
    "print(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:05:52.192594800Z",
     "start_time": "2023-09-30T08:05:52.064161300Z"
    }
   },
   "id": "7127cc256820f766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importo i vari package utili come Tensorflow, numpy, ecc.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13a7b597e4e76c68"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import preprocessing\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import gridspec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:05:57.605064500Z",
     "start_time": "2023-09-30T08:05:56.413933Z"
    }
   },
   "id": "e22aa731ee1f6db6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la funzione di normalizzazione del dataset di training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88766bab7ded85ca"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def normalization(tensor):\n",
    "    tensor = tf.image.resize(tensor, (128, 128))\n",
    "    tensor = tf.subtract(tf.divide(tensor, 127.5), 1)\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:04.530959800Z",
     "start_time": "2023-09-30T08:06:04.499710600Z"
    }
   },
   "id": "c0dd49f267b55cdb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 128, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image, label  in dataset_gloves.take(1):\n",
    "    img = tf.cast(image, tf.float32)\n",
    "    imgs = normalization(img)\n",
    "    print(imgs.shape)\n",
    "    print(label.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:18.197533700Z",
     "start_time": "2023-09-30T08:06:18.008097500Z"
    }
   },
   "id": "4cd65c1942802a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la dimensione latente, hyperparameters importante per la CGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c12855faad13b9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 128\n",
    "latent_dim = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:24.109884600Z",
     "start_time": "2023-09-30T08:06:24.030494Z"
    }
   },
   "id": "fbba019c742b1e7e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 128), dtype=tf.float32, name='input_embedding'), name='input_embedding', description=\"created by layer 'input_embedding'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
     ]
    }
   ],
   "source": [
    "# Verifica le dimensioni\n",
    "con_label = layers.Input(shape=128, name='input_embedding')\n",
    "print(con_label)\n",
    "\n",
    "# latent vector input\n",
    "latent_vector = layers.Input(shape=latent_dim)\n",
    "print(latent_vector)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:25.297148100Z",
     "start_time": "2023-09-30T08:06:25.051758900Z"
    }
   },
   "id": "9693ae227a08b2ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lavoro sulla forma dei due input (etichetta e vettore di rumore) per poter concatenarli tra di loro nel generatore"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b63d9684ea1deb90"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "def label_conditioned_generator():\n",
    "    nodes = 128 * 4 * 4 \n",
    "    label_dense = layers.Dense(nodes)(con_label)\n",
    "    # reshape to additional channel\n",
    "    label_reshape_layer = layers.Reshape((4, 4, 128))(label_dense)\n",
    "    return label_reshape_layer\n",
    "\n",
    "def latent_input(latent_dim=100):\n",
    "    # image generator input\n",
    "    nodes = 512 * 4 * 4\n",
    "    latent_dense = layers.Dense(nodes)(latent_vector)\n",
    "    latent_dense = layers.ReLU()(latent_dense)\n",
    "    latent_reshape = layers.Reshape((4, 4, 512))(latent_dense)\n",
    "    return latent_reshape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:28.247049700Z",
     "start_time": "2023-09-30T08:06:28.223443900Z"
    }
   },
   "id": "71bb467d8554a054"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 8192)                 827392    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " input_embedding (InputLaye  [(None, 128)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 8192)                 0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2048)                 264192    ['input_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 4, 4, 512)            0         ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 4, 4, 128)            0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 4, 4, 640)            0         ['reshape_1[0][0]',           \n",
      "                                                                     'reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv_transpose_1 (Conv2DTr  (None, 8, 8, 512)            5242880   ['concatenate[0][0]']         \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " bn_1 (BatchNormalization)   (None, 8, 8, 512)            2048      ['conv_transpose_1[0][0]']    \n",
      "                                                                                                  \n",
      " relu_1 (ReLU)               (None, 8, 8, 512)            0         ['bn_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv_transpose_2 (Conv2DTr  (None, 16, 16, 256)          2097152   ['relu_1[0][0]']              \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " bn_2 (BatchNormalization)   (None, 16, 16, 256)          1024      ['conv_transpose_2[0][0]']    \n",
      "                                                                                                  \n",
      " relu_2 (ReLU)               (None, 16, 16, 256)          0         ['bn_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv_transpose_3 (Conv2DTr  (None, 32, 32, 128)          524288    ['relu_2[0][0]']              \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " bn_3 (BatchNormalization)   (None, 32, 32, 128)          512       ['conv_transpose_3[0][0]']    \n",
      "                                                                                                  \n",
      " relu_3 (ReLU)               (None, 32, 32, 128)          0         ['bn_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv_transpose_4 (Conv2DTr  (None, 64, 64, 64)           131072    ['relu_3[0][0]']              \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " bn_4 (BatchNormalization)   (None, 64, 64, 64)           256       ['conv_transpose_4[0][0]']    \n",
      "                                                                                                  \n",
      " relu_4 (ReLU)               (None, 64, 64, 64)           0         ['bn_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv_transpose_6 (Conv2DTr  (None, 128, 128, 3)          3072      ['relu_4[0][0]']              \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9093888 (34.69 MB)\n",
      "Trainable params: 9091968 (34.68 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# definisco il modello del generatore CGAN\n",
    "def define_generator():\n",
    "    label_output = label_conditioned_generator()\n",
    "    #label_output = layers.Input(shape=(embedding_resnet.shape))\n",
    "    latent_vector_output= latent_input()\n",
    "    # merge label_conditioned_generator and latent_input output\n",
    "    merge = layers.Concatenate()([latent_vector_output, label_output])\n",
    "\n",
    "    x = layers.Conv2DTranspose(64 * 8, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_1')(merge)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1')(x)\n",
    "    x = layers.ReLU(name='relu_1')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64 * 4, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_2')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2')(x)\n",
    "    x = layers.ReLU(name='relu_2')(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64 * 2, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_3')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_3')(x)\n",
    "    x = layers.ReLU(name='relu_3')(x)\n",
    "\n",
    "\n",
    "    x = layers.Conv2DTranspose(64 * 1, 4, 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_transpose_4')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8,  center=1.0, scale=0.02, name='bn_4')(x)\n",
    "    x = layers.ReLU(name='relu_4')(x)\n",
    "\n",
    "\n",
    "    out_layer = layers.Conv2DTranspose(3, 4, 2,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, activation='tanh', name='conv_transpose_6')(x)\n",
    "\n",
    "   # define model\n",
    "    model = tf.keras.Model([latent_vector, con_label], out_layer)\n",
    "    return model\n",
    "\n",
    "conditional_gen = define_generator()\n",
    "conditional_gen.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:30.253144300Z",
     "start_time": "2023-09-30T08:06:29.624872300Z"
    }
   },
   "id": "569257f19dbd2599"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco l'input del discriminatore: etichetta e immagine del generatore in input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619bbc784937c42d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def label_condition_disc(in_shape=(128,128,3), n_classes=1, embedding_dim=128):\n",
    "    # label input (sarebbe la mia etichetta, cioè bottom,shoes o accesories)\n",
    "    etichetta = layers.Input(shape=(1,), )\n",
    "    label_embedding = layers.Embedding(n_classes, embedding_dim)(etichetta)\n",
    "    # scale up to image dimensions with linear activation\n",
    "    nodes = in_shape[0] * in_shape[1] * in_shape[2]\n",
    "    label_dense = layers.Dense(nodes)(label_embedding)\n",
    "    # reshape to additional channel\n",
    "    label_reshape_layer = layers.Reshape((in_shape[0], in_shape[1], 3))(label_dense)\n",
    "    # image input\n",
    "    return etichetta, label_reshape_layer\n",
    "\n",
    "def image_disc(in_shape=(128,128,3)):\n",
    "    inp_image = layers.Input(shape=in_shape)\n",
    "    return inp_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:36.341454300Z",
     "start_time": "2023-09-30T08:06:36.294440600Z"
    }
   },
   "id": "2003e432bf7dd79c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la funzione del discriminatore CGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e919139caf6cfc5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 1, 128)               128       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1, 49152)             6340608   ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 128, 128, 3)          0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 128, 128, 6)          0         ['input_3[0][0]',             \n",
      " )                                                                   'reshape_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)             (None, 64, 64, 64)           6144      ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_relu_1 (LeakyReLU)    (None, 64, 64, 64)           0         ['conv_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)             (None, 22, 22, 128)          131072    ['leaky_relu_1[0][0]']        \n",
      "                                                                                                  \n",
      " bn_1 (BatchNormalization)   (None, 22, 22, 128)          512       ['conv_2[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_relu_2 (LeakyReLU)    (None, 22, 22, 128)          0         ['bn_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv_3 (Conv2D)             (None, 8, 8, 256)            524288    ['leaky_relu_2[0][0]']        \n",
      "                                                                                                  \n",
      " bn_2 (BatchNormalization)   (None, 8, 8, 256)            1024      ['conv_3[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_relu_3 (LeakyReLU)    (None, 8, 8, 256)            0         ['bn_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv_5 (Conv2D)             (None, 3, 3, 512)            2097152   ['leaky_relu_3[0][0]']        \n",
      "                                                                                                  \n",
      " bn_4 (BatchNormalization)   (None, 3, 3, 512)            2048      ['conv_5[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_relu_5 (LeakyReLU)    (None, 3, 3, 512)            0         ['bn_4[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 4608)                 0         ['leaky_relu_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 4608)                 0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    4609      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9107585 (34.74 MB)\n",
      "Trainable params: 9105793 (34.74 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_discriminator():\n",
    "    etichetta, label_condition_output = label_condition_disc()\n",
    "    inp_image_output = image_disc()\n",
    "    # concat label as a channel\n",
    "    merge = layers.Concatenate(axis=-1)([inp_image_output, label_condition_output])\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides= 2, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_1')(merge)\n",
    "    x = layers.LeakyReLU(0.2, name='leaky_relu_1')(x)\n",
    "\n",
    "    x = layers.Conv2D(64 * 2, kernel_size=4, strides= 3, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_2')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(0.2, name='leaky_relu_2')(x)\n",
    "\n",
    "    x = layers.Conv2D(64 * 4, 4, 3, padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_3')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(0.2, name='leaky_relu_3')(x)\n",
    "\n",
    "    x = layers.Conv2D(64 * 8, 4, 3,padding='same', kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "    mean=0.0, stddev=0.02), use_bias=False, name='conv_5')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.1,  epsilon=0.8, center=1.0, scale=0.02, name='bn_4')(x)\n",
    "    x = layers.LeakyReLU(0.2, name='leaky_relu_5')(x)\n",
    "\n",
    "\n",
    "    flattened_out = layers.Flatten()(x)\n",
    "    # dropout\n",
    "    dropout = layers.Dropout(0.4)(flattened_out)\n",
    "    # output\n",
    "    dense_out = layers.Dense(1, activation='sigmoid')(dropout)\n",
    "    # define model\n",
    "\n",
    "\n",
    "    # define model\n",
    "    model = tf.keras.Model([inp_image_output, etichetta], dense_out)\n",
    "    return model\n",
    "\n",
    "# Inizializziamo il discriminatore CGAN\n",
    "conditional_discriminator = define_discriminator()\n",
    "\n",
    "# Sommario discriminatore CGAN\n",
    "conditional_discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:39.118147900Z",
     "start_time": "2023-09-30T08:06:38.431509900Z"
    }
   },
   "id": "cdf03bdcf2f66ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Come loss scelgo la binary croos entropy, molto utilizzata nelle casistiche di classificazione della CGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec16ad18836807e9"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:44.024944400Z",
     "start_time": "2023-09-30T08:06:43.946754700Z"
    }
   },
   "id": "7c30af9b01bb2c9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la loss del generatore e del discriminatore"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b96c2161f8b7073c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def generator_loss(label, fake_output):\n",
    "    gen_loss = binary_cross_entropy(label, fake_output)\n",
    "    print('gen loss --> ', gen_loss)\n",
    "    return gen_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:45.463686900Z",
     "start_time": "2023-09-30T08:06:45.412216500Z"
    }
   },
   "id": "1a290fe3272b53d0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def discriminator_loss(label, output):\n",
    "    disc_loss = binary_cross_entropy(label, output)\n",
    "    print('disc loss --> ', disc_loss)\n",
    "    return disc_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:46.213557700Z",
     "start_time": "2023-09-30T08:06:46.124810900Z"
    }
   },
   "id": "9764aa8710e0673e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imposto il learning rate e gli ottimizzatori del generatore e del discriminatore, in questo caso utilizzeremo ADAM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1971724eef6a3686"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "larning_rate = 0.00015\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00015, beta_1 = 0.5, beta_2 = 0.999 )\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00015, beta_1 = 0.5, beta_2 = 0.999 )\n",
    "\n",
    "# Per il generatore\n",
    "generator_variables = conditional_gen.trainable_variables\n",
    "\n",
    "# Per il discriminatore\n",
    "discriminator_variables = conditional_discriminator.trainable_variables\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:51.204714300Z",
     "start_time": "2023-09-30T08:06:51.151714200Z"
    }
   },
   "id": "652f8f78b0831c21"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "num_examples_to_generate = 1\n",
    "# Definiamo il seme di generazione del nostro rumore\n",
    "seed = tf.random.normal([num_examples_to_generate, latent_dim])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:52.575201500Z",
     "start_time": "2023-09-30T08:06:52.511815300Z"
    }
   },
   "id": "4f27f38edc503034"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Inizializziamo le variabili per tenere traccia delle somme cumulative\n",
    "total_gen_loss = 0.0\n",
    "total_disc_loss2 = 0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7e47e2a122fd29a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la funzione per il passo di training, quindi tutte le fasi per allenare il modello ad apprendere il pattern"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8862a9029dbd6aed"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Il target è l'etichetta reale dell'immagine\n",
    "def train_step(images,target, G_loss_list, D_loss_list):\n",
    "    \n",
    "    # noise vector sampled from normal distribution\n",
    "    # Crea il vettore di 3 dimensioni con valori casuali da una distribuzione normale\n",
    "    #print('target:',target.shape[0])\n",
    "    noise = tf.random.normal([target.shape[0], latent_dim])\n",
    "    # Train Discriminator with real labels\n",
    "    with tf.GradientTape() as disc_tape1:\n",
    "        # Ottieni la dimensione effettiva del vettore target\n",
    "        dimensione_target = tf.shape(target)[0]\n",
    "        # Modifico la dimensione del batch size dell'embedding in base alla dimensione attuale del batch_size\n",
    "        new_embedding = tf.tile(embedding, [dimensione_target, 1])\n",
    "        # Genero l'immagine fake col gen passando il vettore di rumore e l'embedding dell'immagine\n",
    "        generated_image = conditional_gen([noise,new_embedding], training=True)\n",
    "\n",
    "        real_output = conditional_discriminator([images,target], training=True)\n",
    "        real_targets = tf.ones_like(real_output)\n",
    "        disc_loss1 = discriminator_loss(real_targets, real_output)\n",
    "\n",
    "    # Calcolo del gradiente per il discriminatore per l'etichette reali \n",
    "    gradients_of_disc1 = disc_tape1.gradient(disc_loss1, conditional_discriminator.trainable_variables)\n",
    "\n",
    "    # Ottimizzazione dei parametri del discriminatore per l'etichette reali \n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc1,\\\n",
    "    conditional_discriminator.trainable_variables))\n",
    "\n",
    "    # Training del discriminatore con l'etichette false\n",
    "    with tf.GradientTape() as disc_tape2:\n",
    "        #print('fake output discriminator')\n",
    "        fake_output = conditional_discriminator([generated_image,target], training=True)\n",
    "        fake_targets = tf.zeros_like(fake_output)\n",
    "        disc_loss2 = discriminator_loss(fake_targets, fake_output)\n",
    "        \n",
    "    # Calcolo del gradiente per il discriminatore per l'etichette reali\n",
    "    gradients_of_disc2 = disc_tape2.gradient(disc_loss2, conditional_discriminator.trainable_variables)\n",
    "\n",
    "    # Ottimizzazione dei parametri del discriminatore per l'etichette false\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc2,\\\n",
    "    conditional_discriminator.trainable_variables))\n",
    "\n",
    "    # Fase di Train del Generatore con l'etichette reali\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_image = conditional_gen([noise,new_embedding], training=True)\n",
    "        fake_output = conditional_discriminator([generated_image,target], training=True)\n",
    "        real_targets = tf.ones_like(fake_output)\n",
    "        gen_loss = generator_loss(real_targets, fake_output)\n",
    "\n",
    "    # Calcolo del gradiente per il generatore per l'etichette reali\n",
    "    gradients_of_gen = gen_tape.gradient(gen_loss, conditional_gen.trainable_variables)\n",
    "\n",
    "    # Ottimizzazione dei parametri del generatore per l'etichette reali\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_gen,\\\n",
    "    conditional_gen.trainable_variables))\n",
    "    \n",
    "    G_loss_list.append(gen_loss)\n",
    "    D_loss_list.append(disc_loss2)\n",
    "    \n",
    "    # Accumiamo le losses per ogni step\n",
    "    global total_gen_loss\n",
    "    global total_disc_loss2\n",
    "    total_gen_loss += gen_loss\n",
    "    total_disc_loss2 += disc_loss2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:06:59.011577600Z",
     "start_time": "2023-09-30T08:06:58.980281700Z"
    }
   },
   "id": "852a9b2631d08259"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo le funzione per il calcolo del FID e Inception Score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6991d1365a94113b"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-probability in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (1.23.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (0.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (4.5.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (0.1.8)\n",
      "Requirement already satisfied: absl-py in c:\\users\\marce\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-probability) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "from keras.src.utils import img_to_array, load_img\n",
    "!pip install tensorflow-probability\n",
    "\n",
    "# Importa librerie e modelli necessari\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Importazioni delle librerie e dei modelli necessari\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Funzione per estrarre le features da un set di immagini utilizzando il modello InceptionV3\n",
    "    \n",
    "def inception_features(images):\n",
    "    # Carica il modello InceptionV3 preaddestrato\n",
    "    base_model = InceptionV3(weights='imagenet', input_shape=(299, 299, 3))\n",
    "    model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "    # Ingrandisci e pre-elabora le immagini\n",
    "    img = tf.image.resize(images, (299, 299))\n",
    "    preprocessed_images = preprocess_input(img)\n",
    "\n",
    "    # Utilizza il modello InceptionV3 per estrarre le features dalle immagini\n",
    "    features = model.predict(preprocessed_images)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# Funzione per calcolare il FID\n",
    "# Funzione per calcolare il FID tra due insiemi di immagini\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    # Calcola le statistiche delle feature dalle immagini reali e generate\n",
    "    real_features = inception_features(real_images)\n",
    "    generated_features = inception_features(generated_images)\n",
    "\n",
    "    # Calcola la media e la covarianza delle feature delle immagini reali e generate\n",
    "    mu_real, sigma_real = tf.math.reduce_mean(real_features, axis=0), tfp.stats.covariance(real_features)\n",
    "    mu_generated, sigma_generated = tf.math.reduce_mean(generated_features, axis=0), tfp.stats.covariance(generated_features)\n",
    "\n",
    "    # Calcola la traccia della radice quadrata delle matrici di covarianza\n",
    "    sqrt_term = sqrtm(sigma_real @ sigma_generated)\n",
    "\n",
    "    # Calcola la differenza tra le medie delle feature\n",
    "    diff = mu_real - mu_generated\n",
    "\n",
    "    # Calcola la distanza di Fréchet\n",
    "    # Ho modificato la normalizzazione da \"frobenius\" che è una norma non supportata da tensorflow in \"euclidean\"\n",
    "    fid = tf.math.real(tf.linalg.trace(sigma_real + sigma_generated - 2.0 * sqrt_term)) + tf.norm(diff, ord='euclidean')**2\n",
    "    return fid\n",
    "\n",
    "\n",
    "# Funzione per calcolare l'Inception Score\n",
    "def calculate_inception_score(generated_images):\n",
    "    # Carica il modello InceptionV3 preaddestrato senza gli strati finali\n",
    "    inception_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Estrai le feature dalle immagini generate\n",
    "    features = inception_model.predict(generated_images)\n",
    "\n",
    "    # Calcola le previsioni di classe per ogni immagine\n",
    "    class_predictions = tf.nn.softmax(features, axis=-1)\n",
    "\n",
    "    # Calcola l'entropia delle previsioni di classe\n",
    "    entropy = -tf.reduce_sum(class_predictions * tf.math.log(class_predictions + 1e-10), axis=-1)\n",
    "\n",
    "    # Calcola l'Inception Score come media delle entropie delle previsioni di classe\n",
    "    inception_score = tf.math.exp(tf.reduce_mean(entropy))\n",
    "    return inception_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:07:06.917976500Z",
     "start_time": "2023-09-30T08:07:01.027647900Z"
    }
   },
   "id": "cb1c26518da4e298"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Funzione per valutare il modello utilizzando FID e Inception Score\n",
    "def evaluate_model(conditional_gen, validation_dataset):\n",
    "    fid_scores = []\n",
    "    inception_scores = []\n",
    "\n",
    "    # Itera attraverso il set di dati di validazione\n",
    "    for batch_images, batch_targets in validation_dataset:\n",
    "        img = tf.cast(batch_images, tf.float32)\n",
    "        imgs = normalization(img)\n",
    "        \n",
    "        # Genera le immagini condizionalmente\n",
    "        noise = tf.random.normal([batch_targets.shape[0], latent_dim])\n",
    "        dimensione_target = tf.shape(batch_targets)[0]\n",
    "        new_embedding = tf.tile(embedding, [dimensione_target, 1])\n",
    "        generated_images = conditional_gen([noise, new_embedding], training=False)\n",
    "\n",
    "        # Calcola il FID tra le immagini reali e generate utilizzando il validation_dataset\n",
    "        print('imgs shape', imgs.shape)\n",
    "        fid = calculate_fid(imgs, generated_images)\n",
    "        fid_scores.append(fid)\n",
    "\n",
    "        # Calcola l'Inception Score per le immagini generate\n",
    "        inception_score = calculate_inception_score(generated_images)\n",
    "        inception_scores.append(inception_score)\n",
    "\n",
    "    # Calcola la media dei punteggi FID e Inception Score\n",
    "    average_fid = np.mean(fid_scores)\n",
    "    average_inception_score = np.mean(inception_scores)\n",
    "\n",
    "    return average_fid, average_inception_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:07:07.168989900Z",
     "start_time": "2023-09-30T08:07:07.090816600Z"
    }
   },
   "id": "a425278bb21b3e3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo la funzione di train che itera sull'intero dataset impostando le epoche. All'interno, ogni tot epoche verrà effettuata la valutazione del modello tramite FID e INCEPTION score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dfdcc1563761d20"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def train(dataset, epochs, validation_dataset):\n",
    "    \n",
    "    # Verifica se la directory per il salvataggio dei pesi esiste, altrimenti creala\n",
    "    save_dir = 'Dataset/training_weights/Accessories/Gloves/Params_Tuning/lr_batch'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    #Definisci la frequenza di valutazione sul set di validazione (ad esempio, ogni 20 epoche)\n",
    "    validation_interval = 20\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        D_loss_list, G_loss_list = [], []\n",
    "        fid_list, inception_list = [], []\n",
    "        # Ciclo for che attraversa il mio dataset con dataset che è una collezione di coppie di dati\n",
    "        for image, label in dataset:\n",
    "            # Non devo filtrare niente perchè il dataset è formato da sole immagini di tops\n",
    "                i += 1\n",
    "                img = tf.cast(image, tf.float32)\n",
    "                imgs = normalization(img)\n",
    "                train_step(imgs, label, G_loss_list, D_loss_list)\n",
    "        # Valuta il modello sul set di validazione alla fine di ogni epoca\n",
    "        if (epoch + 1) % validation_interval == 0:\n",
    "            average_fid, average_inception_score = evaluate_model(conditional_gen, validation_dataset)\n",
    "            print(f'Epoch {epoch + 1} - Average FID: {average_fid}, Average Inception Score: {average_inception_score}')\n",
    "            fid_list.append(average_fid)\n",
    "            inception_list.append(average_inception_score)\n",
    "        #display.clear_output(wait=True)\n",
    "        generate_and_save_images(conditional_gen, epoch + 1, seed, save_dir='/content/train/')\n",
    "\n",
    "        # Salva i pesi del modello ogni 20 epoche\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            conditional_gen.save_weights('Dataset/training_weights/Accessories/Gloves/Params_Tuning/lr_batch/gen_'+ str(epoch)+'.h5')\n",
    "            conditional_discriminator.save_weights('Dataset/training_weights/Accessories/Gloves/Params_Tuning/lr_batch/disc_'+ str(epoch)+'.h5')\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Genera dopo l'ultima epoca\n",
    "    #display.clear_output(wait=True)\n",
    "    generate_and_save_images(conditional_gen,\n",
    "                            epochs,\n",
    "                            seed, save_dir='/content/train/')\n",
    "    \n",
    "    print('Fid List : ', fid_list)\n",
    "    print('Inception list: ', inception_list)\n",
    "    \n",
    "    avg_gen_loss = total_gen_loss / epochs  \n",
    "    avg_disc_loss2 = total_disc_loss2 / epochs  \n",
    "\n",
    "    print(\"Average Generator Loss:\", avg_gen_loss)\n",
    "    print(\"Average Discriminator Loss2:\", avg_disc_loss2)\n",
    "    \n",
    "    return D_loss_list, G_loss_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:07:29.770139900Z",
     "start_time": "2023-09-30T08:07:29.738823400Z"
    }
   },
   "id": "ac5677ff3e399f8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la funzione per generare ogni immagine di ogni epoca e salvarla nel percorso"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d71fb4af01d15273"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test input --> seed\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input, save_dir):\n",
    "    # Creare la directory se non esiste\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # nota che `training` è settato su False.\n",
    "    # Questo perchè tutti i layers vengono eseguiti in inference mode (batchnorm).\n",
    "\n",
    "    # Io al generatore non passo l'etichetta tops,bottoms,shoes,ecc ma gli passo il mio embedding\n",
    "\n",
    "    predictions = model([test_input, embedding], training=False)\n",
    "    print(predictions.shape)\n",
    "    fig = plt.figure(figsize=(25, 25))\n",
    "\n",
    "    print(\"Generated Images are Conditioned on Embedding of Tops:\")\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        pred = (predictions[i, :, :, :] + 1) * 127.5\n",
    "        pred = np.array(pred)\n",
    "        plt.imshow(pred.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Salvare l'immagine nella directory specificata\n",
    "    image_path = os.path.join(save_dir, 'image_at_epoch_{:d}.png'.format(epoch))\n",
    "    plt.savefig(image_path)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0444da146cfef6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definisco la funzione per generare e visualizzare graficamente l'ultima immagine del training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6b33bb4fbfe3ec3"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    # Visualizza un'immagine senza griglia.\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def generate_and_display_image(model, test_input):\n",
    "    # Imposta training su False per l'inferenza.\n",
    "    output = model([test_input, embedding], training=False)\n",
    "\n",
    "    # Normalizza e converte l'output in un array numpy di immagine.\n",
    "    pred = (output[0, :, :, :] + 1 ) * 127.5\n",
    "    pred = np.array(pred, dtype=np.uint8)\n",
    "\n",
    "    # Visualizza l'immagine risultato.\n",
    "    display_image(pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:07:54.541540500Z",
     "start_time": "2023-09-30T08:07:54.416025300Z"
    }
   },
   "id": "9ed1be20e0b9227c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effettuiamo il training con lr=0.0002 e batch_size = 64"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14cbe7883669ce33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(dataset_gloves, 60, dataset_gloves_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cac9a13e6bc09f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training con lr=0.00015 e lat_dim = 100 e batch_size=32"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f810e99f5cfcdd59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(dataset_gloves, 80, dataset_gloves_val)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d67fa713513ad7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ricarichiamo i pesi del training precedente per poter valutare le prestazioni sul test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7b534e60bd28fa"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "conditional_gen.load_weights('Dataset/training_weights/Accessories/Gloves/Params_Tuning/lr_batch/gen_79.h5')\n",
    "conditional_discriminator.load_weights('Dataset/training_weights/Accessories/Gloves/Params_Tuning/lr_batch/disc_79.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:08:04.554849Z",
     "start_time": "2023-09-30T08:08:04.342269500Z"
    }
   },
   "id": "5b4af657e4dd110c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tramite la funzione evaluate_model valutiamo il modello allenato sul test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ec0f1a8783fd809"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs shape (32, 128, 128, 3)\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:12924: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _result = pywrap_tfe.TFE_Py_FastPathExecute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "imgs shape (32, 128, 128, 3)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FDA1A2FEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD8339C430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "imgs shape (32, 128, 128, 3)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "imgs shape (32, 128, 128, 3)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "imgs shape (32, 128, 128, 3)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "imgs shape (24, 128, 128, 3)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      " - Average FID: 42.812320709228516, Average Inception Score: 56.60957717895508\n"
     ]
    }
   ],
   "source": [
    "average_fid, average_inception_score = evaluate_model(conditional_gen, dataset_gloves_test)\n",
    "print(f' - Average FID: {average_fid}, Average Inception Score: {average_inception_score}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T08:27:05.180672100Z",
     "start_time": "2023-09-29T08:24:35.393687200Z"
    }
   },
   "id": "564aaffa279fa0c7"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer 'model_1' (type Functional).\n\n{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported [Op:Cast] name: \n\nCall arguments received by layer 'model_1' (type Functional):\n  • inputs=['tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=string)']\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnimplementedError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_gloves\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_gloves_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dataset, epochs, validation_dataset)\u001B[0m\n\u001B[0;32m     19\u001B[0m         img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(image, tf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     20\u001B[0m         imgs \u001B[38;5;241m=\u001B[39m normalization(img)\n\u001B[1;32m---> 21\u001B[0m         \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mG_loss_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mD_loss_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Valuta il modello sul set di validazione alla fine di ogni epoca\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m validation_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[21], line 16\u001B[0m, in \u001B[0;36mtrain_step\u001B[1;34m(images, target, G_loss_list, D_loss_list)\u001B[0m\n\u001B[0;32m     12\u001B[0m new_embedding \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mtile(embedding, [dimensione_target, \u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     14\u001B[0m generated_image \u001B[38;5;241m=\u001B[39m conditional_gen([noise,new_embedding], training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 16\u001B[0m real_output \u001B[38;5;241m=\u001B[39m \u001B[43mconditional_discriminator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m real_targets \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mones_like(real_output)\n\u001B[0;32m     18\u001B[0m disc_loss1 \u001B[38;5;241m=\u001B[39m discriminator_loss(real_targets, real_output)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   6654\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name):\n\u001B[0;32m   6655\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m-> 6656\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mUnimplementedError\u001B[0m: Exception encountered when calling layer 'model_1' (type Functional).\n\n{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported [Op:Cast] name: \n\nCall arguments received by layer 'model_1' (type Functional):\n  • inputs=['tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)', 'tf.Tensor(shape=(32,), dtype=string)']\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "train(dataset_gloves, 30, dataset_gloves_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T08:08:28.827391200Z",
     "start_time": "2023-09-30T08:08:26.549930Z"
    }
   },
   "id": "d1e94de6633a8725"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
