{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In questo notebook andremo a preprocessare tutti i file JSON per creare dei dataset di TRAIN, VALIDATION e TEST utili e pronti per il progetto"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7be9a5d5499fef1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Questa funzione aggiunge l'attributo semantic_category del file metadata nel file train, validation o test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba10f1eba81316b4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json\n",
    "def aggiungi_categorie_semantiche(file1_path, file2_path, output_path):\n",
    "    # Carica il contenuto del primo file JSON (descrizione degli outfit)\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        data1 = json.load(file1)\n",
    "\n",
    "    # Carica il contenuto del secondo file JSON\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        data2 = json.load(file2)\n",
    "\n",
    "    # Crea un dizionario di mapping item_id -> semantic_category dal secondo file\n",
    "    id_to_category = {item_id: item[\"semantic_category\"] for item_id, item in data2.items()}\n",
    "\n",
    "    # Itera attraverso i dizionari nella lista data1\n",
    "    for item_dict in data1:\n",
    "        # Accedi all'array \"items\" all'interno del dizionario corrente\n",
    "        items_list = item_dict[\"items\"]\n",
    "\n",
    "        # Aggiungi le semantic_category agli oggetti nell'array \"items\"\n",
    "        for item in items_list:\n",
    "            item_id = item[\"item_id\"]\n",
    "            semantic_category = id_to_category.get(item_id)\n",
    "            if semantic_category:\n",
    "                item[\"semantic_category\"] = semantic_category\n",
    "\n",
    "    # Salva il nuovo contenuto nel primo file JSON aggiornato\n",
    "    with open(output_path, 'w') as file1_updated:\n",
    "        json.dump(data1, file1_updated, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:06:27.976385400Z",
     "start_time": "2023-09-29T16:06:27.944684300Z"
    }
   },
   "id": "69c2a66ac2d6350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Richiamiamo la funzione per i 3 set (training, validation e test) \n",
    "Ci saranno 2 tipologie di \"gruppi\" di set, 1 gruppo uguale per la categoria bottom e shoes, e 1 gruppo per gli accessori che necessitano dell'attributo category_id per lavorare"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d143e96a5b775166"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per il train set:\n",
    "file1_path = 'C:/Users\\marce\\Desktop\\polyvore_outfits/disjoint/train.json'\n",
    "file2_path = 'C:/Users\\marce\\Desktop\\polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/train_set_bs.json'\n",
    "aggiungi_categorie_semantiche(file1_path, file2_path, output_path)\n",
    "\n",
    "# Lavoriamo nello stesso modo per preprocessare il validation set\n",
    "file1_path = 'C:/Users/marce/Desktop/polyvore_outfits/disjoint/valid.json'\n",
    "file2_path = 'C:/Users/marce/Desktop/polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/valid_set_bs.json'\n",
    "aggiungi_categorie_semantiche(file1_path, file2_path, output_path)\n",
    "\n",
    "# Lavoriamo nello stesso modo per preprocessare il test set\n",
    "file1_path = 'C:/Users/marce/Desktop/polyvore_outfits/disjoint/test.json'\n",
    "file2_path = 'C:/Users/marce/Desktop/polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/test_set_bs.json'\n",
    "aggiungi_categorie_semantiche(file1_path, file2_path, output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:06:41.494887200Z",
     "start_time": "2023-09-29T16:06:30.887051Z"
    }
   },
   "id": "233907d9ea2c1df2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminiamo gli outfit che hanno due items con la stessa categoria"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cb848169847b49e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def filtra_dati_e_salva(input_file_path, output_file_path):\n",
    "    # Carica il file JSON\n",
    "    with open(input_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Nuova struttura dati con elementi filtrati\n",
    "    filtered_data = []\n",
    "\n",
    "    # Itera attraverso gli elementi nel JSON\n",
    "    for entry in json_data:\n",
    "        new_entry = {\n",
    "            \"items\": [],\n",
    "            \"set_id\": entry[\"set_id\"]\n",
    "        }\n",
    "\n",
    "        # Dizionario per tenere traccia delle categorie semantiche per ogni \"item\"\n",
    "        item_semantic_categories = {}\n",
    "\n",
    "        for item in entry[\"items\"]:\n",
    "            category = item[\"semantic_category\"]\n",
    "            if category not in item_semantic_categories:\n",
    "                item_semantic_categories[category] = True\n",
    "                new_entry[\"items\"].append(item)\n",
    "\n",
    "        filtered_data.append(new_entry)\n",
    "\n",
    "    # Sovrascrive il file JSON originale con i dati filtrati\n",
    "    with open(output_file_path, 'w') as json_file:\n",
    "        json.dump(filtered_data, json_file, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:07:14.695821200Z",
     "start_time": "2023-09-29T16:07:14.648409800Z"
    }
   },
   "id": "d92597f3558e3159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eseguiamo la funzione sui 3 set "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91515c01bfad35d9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per il train set:\n",
    "input_file_path = 'Dataset/train_set_bs.json'\n",
    "output_file_path = 'Dataset/train_set_bs.json'\n",
    "filtra_dati_e_salva(input_file_path, output_file_path)\n",
    "\n",
    "# Facciamo la stessa cosa per il validation set\n",
    "output_path = 'Dataset/valid_set_bs.json'\n",
    "filtra_dati_e_salva(output_path,'Dataset/valid_set_bs.json')\n",
    "\n",
    "#Idem per il test set \n",
    "output_path = 'Dataset/test_set_bs.json'\n",
    "filtra_dati_e_salva(output_path,'Dataset/test_set_bs.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:07:22.920390600Z",
     "start_time": "2023-09-29T16:07:19.153141Z"
    }
   },
   "id": "ce6f200fd156790c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Questa funzione elimina gli outfit con meno di 4 elementi al proprio interno"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1478ab21925eeaf"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def outfit_con_4_elementi(input_file_path, output_file_path, min_item_count=4):\n",
    "    # Carica il file JSON\n",
    "    with open(input_file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Nuova struttura dati con elementi filtrati\n",
    "    filtered_data = []\n",
    "\n",
    "    # Itera attraverso gli elementi nel JSON\n",
    "    for entry in json_data:\n",
    "        if len(entry[\"items\"]) >= min_item_count:\n",
    "            filtered_items = [item for item in entry[\"items\"] if item]\n",
    "            if len(filtered_items) >= min_item_count:\n",
    "                new_entry = {\n",
    "                    \"items\": filtered_items,\n",
    "                    \"set_id\": entry[\"set_id\"]\n",
    "                }\n",
    "                filtered_data.append(new_entry)\n",
    "\n",
    "    # Sovrascrive il file JSON originale con i dati filtrati\n",
    "    with open(output_file_path, 'w') as json_file:\n",
    "        json.dump(filtered_data, json_file, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:07:37.354887800Z",
     "start_time": "2023-09-29T16:07:37.307147300Z"
    }
   },
   "id": "b609b4d45ccfba79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eseguiamo la funzione sui 3 set (che verranno utilizzati da Bottoms e Shoes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8812d46c8243c49"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per il training set:\n",
    "input_file_path = 'Dataset/train_set_bs.json'\n",
    "output_file_path = 'Dataset/train_set_bs.json'\n",
    "outfit_con_4_elementi(input_file_path, output_file_path)\n",
    "\n",
    "# Utilizzo della funzione per il validation set:\n",
    "input_file_path = 'Dataset/valid_set_bs.json'\n",
    "output_file_path = 'Dataset/valid_set_bs.json'\n",
    "outfit_con_4_elementi(input_file_path, output_file_path)\n",
    "\n",
    "# Utilizzo della funzione per il validation set:\n",
    "input_file_path = 'Dataset/test_set_bs.json'\n",
    "output_file_path = 'Dataset/test_set_bs.json'\n",
    "outfit_con_4_elementi(input_file_path, output_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:07:48.574910600Z",
     "start_time": "2023-09-29T16:07:45.224576700Z"
    }
   },
   "id": "9515f9e44936cf5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Questa funzione è un \"upgrade\" della funzione \"aggiungi_categorie_semantiche\". La novità è che aggiunge nel file train/valid e test .json l'attributo \"category_id\" corrispondente per ogni categoria semantica"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce98a908693af0d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import json\n",
    "def aggiungi_categorie_semantiche_acc(file1_path, file2_path, output_path):\n",
    "    # Carica il contenuto del primo file JSON (descrizione degli outfit)\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        data1 = json.load(file1)\n",
    "\n",
    "    # Carica il contenuto del secondo file JSON\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        data2 = json.load(file2)\n",
    "\n",
    "    # Crea un dizionario di mapping item_id -> {\"semantic_category\", \"category_id\"} dal secondo file\n",
    "    id_to_info = {item_id: {\"semantic_category\": item[\"semantic_category\"], \"category_id\": item.get(\"category_id\")} for item_id, item in data2.items()}\n",
    "\n",
    "    # Itera attraverso i dizionari nella lista data1\n",
    "    for item_dict in data1:\n",
    "        # Accedi all'array \"items\" all'interno del dizionario corrente\n",
    "        items_list = item_dict[\"items\"]\n",
    "\n",
    "        # Aggiungi le semantic_category e category_id agli oggetti nell'array \"items\"\n",
    "        for item in items_list:\n",
    "            item_id = item[\"item_id\"]\n",
    "            info = id_to_info.get(item_id)\n",
    "            if info:\n",
    "                item[\"semantic_category\"] = info[\"semantic_category\"]\n",
    "                item[\"category_id\"] = info[\"category_id\"]\n",
    "\n",
    "    # Salva il nuovo contenuto nel primo file JSON aggiornato\n",
    "    with open(output_path, 'w') as file1_updated:\n",
    "        json.dump(data1, file1_updated, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:29:22.157599200Z",
     "start_time": "2023-09-29T16:29:22.093357100Z"
    }
   },
   "id": "830f6b11987e2cf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lavoro col file train della cartella nondisjoint perchè alcune category_id come per esempio 52, che rappresenta le cinture, non è presente all'interno del file train della cartella disjoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9d88e8e5ad038b9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Utilizzo della funzione per il training set di accessories:\n",
    "file1_path = 'C:/Users/marce/Desktop/polyvore_outfits/nondisjoint/train.json'\n",
    "file2_path = 'C:/Users/marce/Desktop/polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/train_set_acc.json'\n",
    "aggiungi_categorie_semantiche_acc(file1_path, file2_path, output_path)\n",
    "\n",
    "# Utilizzo della funzione per il validation set di accessories:\n",
    "file1_path = 'C:/Users/marce/Desktop/polyvore_outfits/nondisjoint/valid.json'\n",
    "file2_path = 'C:/Users/marce/Desktop/polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/valid_set_acc.json'\n",
    "aggiungi_categorie_semantiche_acc(file1_path, file2_path, output_path)\n",
    "\n",
    "# Utilizzo della funzione per il test set di accessories:\n",
    "file1_path = 'C:/Users/marce/Desktop/polyvore_outfits/nondisjoint/test.json'\n",
    "file2_path = 'C:/Users/marce/Desktop/polyvore_outfits/polyvore_item_metadata.json'\n",
    "output_path = 'Dataset/test_set_acc.json'\n",
    "aggiungi_categorie_semantiche_acc(file1_path, file2_path, output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:29:40.729596600Z",
     "start_time": "2023-09-29T16:29:24.809280700Z"
    }
   },
   "id": "69e4276b89bb92af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dal file csv categories.csv recupero solo le righe che hanno come semantic_categories accessories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2775bac995189b4f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le righe con 'accessories' sono state scritte su categories_accessories.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Apri il file CSV di input in modalità di lettura\n",
    "with open('C:/Users/marce/Desktop/polyvore_outfits/categories.csv', 'r') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    \n",
    "    # Crea una lista per le righe che soddisfano il criterio\n",
    "    selected_rows = [row for row in reader if row[-1] == 'accessories']\n",
    "\n",
    "# Nome del file CSV di output\n",
    "output_file = 'categories_accessories.csv'\n",
    "\n",
    "# Scrivi le righe selezionate nel nuovo file CSV\n",
    "with open(output_file, 'w', newline='') as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerows(selected_rows)\n",
    "\n",
    "print(f\"Le righe con 'accessories' sono state scritte su {output_file}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:29:56.926634600Z",
     "start_time": "2023-09-29T16:29:56.847943Z"
    }
   },
   "id": "86aee25244044930"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampo a video la tabella csv delle tipologie di accessories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f389385697dce5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      53                       gloves  accessories\n",
      "0     56                          tie  accessories\n",
      "1     58                     headband  accessories\n",
      "2     59                     umbrella  accessories\n",
      "3     68                    stockings  accessories\n",
      "4     69                        socks  accessories\n",
      "5    231                        pouch  accessories\n",
      "6    251                      hosiery  accessories\n",
      "7    269                   suspenders  accessories\n",
      "8    270           activity wristband  accessories\n",
      "9    299             male accessories  accessories\n",
      "10   300                    male belt  accessories\n",
      "11   301                  male gloves  accessories\n",
      "12   302                 male scarves  accessories\n",
      "13   306              male suspenders  accessories\n",
      "14  4460                   male socks  accessories\n",
      "15  4463                       wallet  accessories\n",
      "16  4467                    key chain  accessories\n",
      "17  4468                   money clip  accessories\n",
      "18  4470                       bowtie  accessories\n",
      "19  4472  smartphone cases/tech items  accessories\n",
      "20  4473                     umbrella  accessories\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "# Stampa la tabella CSV a video\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:30:01.598676200Z",
     "start_time": "2023-09-29T16:30:01.003801800Z"
    }
   },
   "id": "23d0d30f5487137"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora andiamo a sostituire il valore intero dell'attributo \"category_id\" con il secondo valore della riga corrispondente nel file csv categories.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aeceec36a0c3c67"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def replace_category_ids(input_json_path, output_json_path):\n",
    "    # Carica il file JSON\n",
    "    with open(input_json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Crea un dizionario per mappare i valori di \"category_id\" con i valori dalla colonna CSV\n",
    "    category_mapping = {}\n",
    "    with open('categories_accessories.csv', 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            category_id = row[0]\n",
    "            category_value = row[1]\n",
    "            category_mapping[category_id] = category_value\n",
    "\n",
    "    # Funzione per sostituire il valore di \"category_id\"\n",
    "    def replace_category_id(item):\n",
    "        category_id = item.get(\"category_id\")\n",
    "        if category_id == \"52\":\n",
    "            item[\"category_id\"] = \"belt\"\n",
    "        elif category_id in category_mapping:\n",
    "            item[\"category_id\"] = category_mapping[category_id]\n",
    "\n",
    "    # Applica la sostituzione ai dati JSON\n",
    "    for entry in data:\n",
    "        for item in entry[\"items\"]:\n",
    "            replace_category_id(item)\n",
    "\n",
    "    # Salva i dati modificati in un nuovo file JSON\n",
    "    with open(output_json_path, 'w') as new_json_file:\n",
    "        json.dump(data, new_json_file, indent=4)\n",
    "\n",
    "    print(\"Sostituzione completata.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:30:12.532825500Z",
     "start_time": "2023-09-29T16:30:12.501374600Z"
    }
   },
   "id": "d87a37f60f9ae5a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sostituzione completata.\n",
      "Sostituzione completata.\n",
      "Sostituzione completata.\n"
     ]
    }
   ],
   "source": [
    "# utilizzo della funzione per il training set di accessories\n",
    "replace_category_ids('Dataset/train_set_acc.json', 'Dataset/train_set_acc.json')\n",
    "\n",
    "# utilizzo della funzione per il validation set di accessories\n",
    "replace_category_ids('Dataset/valid_set_acc.json', 'Dataset/valid_set_acc.json')\n",
    "\n",
    "# utilizzo della funzione per il test set di accessories\n",
    "replace_category_ids('Dataset/test_set_acc.json', 'Dataset/test_set_acc.json')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:30:25.046453200Z",
     "start_time": "2023-09-29T16:30:15.524890500Z"
    }
   },
   "id": "87047c42472ed87d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora possiamo effettuare le stesse operazioni di preprocessing dell'altro gruppo di dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4edfded64156319"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Per il training set di accessories\n",
    "filtra_dati_e_salva('Dataset/train_set_acc.json','Dataset/train_set_acc.json')\n",
    "# Per il validation set di accessories\n",
    "filtra_dati_e_salva('Dataset/valid_set_acc.json','Dataset/valid_set_acc.json')\n",
    "# Per il test set di accessories\n",
    "filtra_dati_e_salva('Dataset/test_set_acc.json', 'Dataset/test_set_acc.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:31:17.484861700Z",
     "start_time": "2023-09-29T16:31:08.557655200Z"
    }
   },
   "id": "1576aa29e7854b98"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Per il training set di accessories\n",
    "outfit_con_4_elementi('Dataset/train_set_acc.json','Dataset/train_set_acc.json')\n",
    "# Per il validation set di accessories\n",
    "outfit_con_4_elementi('Dataset/valid_set_acc.json','Dataset/valid_set_acc.json')\n",
    "# Per il test set di accessories\n",
    "outfit_con_4_elementi('Dataset/test_set_acc.json', 'Dataset/test_set_acc.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:31:26.492263600Z",
     "start_time": "2023-09-29T16:31:18.462551300Z"
    }
   },
   "id": "b944886866fab7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determino la funzione della creazione dell'embedding partendo da un'immagine di un top tramite Residual NN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6479242b30fc7c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def get_image_embedding(image_id):\n",
    "    # Carico il modello RESNet50 pre-allenato senza l'ultimo classification layer\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "\n",
    "    # Aggiungo uno strato di Global Average Pooling per ridurre le dimensioni dell'output\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Aggiungo un FC layer con 128 unità per ottenere un'embedding di dimensioni (1, 128)\n",
    "    embedding_layer = tf.keras.layers.Dense(128)(x)\n",
    "\n",
    "    # Costruisco il nuovo modello\n",
    "    model = Model(inputs=base_model.input, outputs=embedding_layer)\n",
    "    \n",
    "    # Carica l'immagine\n",
    "    img_path = f'C:/Users/marce/Desktop/polyvore_outfits/images/{image_id}.jpg'  # Utilizzo l'image_id passato come argomento\n",
    "    img = image.load_img(img_path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    # Mostra l'immagine\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Rimuovi gli assi\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcola l'embedding\n",
    "    embedding = model.predict(x)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Utilizzo la funzione per ottenere un'embedding di dimensioni (1, 128) di un'immagine specifica\n",
    "#Abbiamo scelto questa immagine del top scorrendo le varie immagini nella cartella images\n",
    "image_id = 178260114\n",
    "\n",
    "embedding = get_image_embedding(image_id)\n",
    "\n",
    "# Visualizza l'embedding\n",
    "print(\"Embedding dell'immagine tops: \", embedding)\n",
    "\n",
    "# Visualizza le dimensioni\n",
    "print(\"Dimensioni embedding: \", embedding.shape)\n",
    "\n",
    "# Salva l'embedding permanentemente in un file NumPy\n",
    "np.save('embedding.npy', embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
